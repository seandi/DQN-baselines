[GENERAL]
disable_cuda = False

[TRAINING]
# Number of interaction steps during training.
# N train steps = N interactions * train freq
interaction_steps = 200000
# Max number of episodes fro training. Set to None to disable
max_episodes = None
# Every this interaction steps do one train steps
train_agent_frequency = 1
# Save the network models every this TRAIN steps
save_model_interval = 100000
# Epsilon hyper-parameters
epsilon_start = 1.0
epsilon_min = 0.04
# either exponential or linear
epsilon_scheduler = linear
# used for exponential schedule
epsilon_decay_factor = None
# used for linear schedule
epsilon_decay_period = 40000

# Evaluate the agent during training every this number of interactions
# If set to None the agent is evaluated only at the end
eval_interval = None
eval_episodes = 30
# During training actions are chosen with epsilon-greedy
# according to this eps value (possibly zero)
eval_epsilon = 0.02

# Start to optimize the agent after this interaction steps
start_optimizing_agent_after = 1000


[AGENT]
learning_rate = 0.001
batch_size = 128
gamma = 0.99
buffer_size = 10000
# Available optimizer: Adam
optimizer = Adam
# Update the target dqn every this interaction steps
# This means that the two nets will be synced every
# INTERCATION_STEPS/TRAIN_FREQ training steps.
update_target_net_params = 1000
model = MLP_DQN
net_arch = [128, 128]
